# Gradient descent in an algorithm which will help you determine the direction with optimal learning rate to iterate to find the best combination of weights which will result in minimum error.
# And the speed of the iteration is determined by the learning rate. Higher the learning rate lesser the chance to find the approximate value. Smaller the learning rate, higher the chances of finding the best approximate value.

# Gradient descent = x - learning_rate * dy/dx
# mean_squared_error= (actual_value - predicted_value )sqr / number_of_predication
# mean_absolute_error = (| actual-value - predicted_value | )/number_of_predication

# Stochastic Gradient Descent : 1 training data at a time
# mini Gradient Descent : n training data at a time 
# Gradient Descent : full data set at a time

# https://ml-cheatsheet.readthedocs.io/en/latest/index.html

